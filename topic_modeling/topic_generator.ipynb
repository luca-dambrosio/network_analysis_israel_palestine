{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b823572-75f0-48b3-870d-7c50085f6ce6",
   "metadata": {},
   "source": [
    "# Install and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d62b429-1f73-4c98-9184-60b9d5cc13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries\n",
    "## matplotlib inline\n",
    "## pip install emoji\n",
    "## pip install bertopic\n",
    "## pip install umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb92bc7d-9026-4d92-85f2-7e9bd708070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import networkx as nx\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# For Cleaning and Topic Modeling\n",
    "import spacy\n",
    "import emoji\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from bertopic import BERTopic\n",
    "# Initialize the umap model, mostly to set the seed\n",
    "from umap import UMAP\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, \n",
    "                  min_dist=0.0, metric='cosine', random_state=42)\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.rcParams.update({'font.size': 10, 'font.style': 'normal', 'font.family':'serif'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49cb5a5a-0f6a-42ae-9b23-18771441d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataframe\n",
    "data = pd.read_json(\"network_data.json\")\n",
    "# Convert columns to datetime format\n",
    "data.loc[:, 'son_date'] = pd.to_datetime(data['son_date'])\n",
    "data.loc[:, 'mother_date'] = pd.to_datetime(data['mother_date'])\n",
    "# Open and read the JSON file containing communities as keys and authors composing them as values\n",
    "with open(\"communities2023-10-07_2023-12-31.json\", 'r') as file:\n",
    "    communities = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53385d9a-d253-458f-8142-88e9eaa58ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for k in communities.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fb947-dac9-473a-bcd0-665778a89033",
   "metadata": {},
   "source": [
    "# Functions to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c55f512-c9d3-4550-9b27-8c6fefa44a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the texts from the dataframe\n",
    "def extract_texts(communities, df_tm, verbose=True):\n",
    "    '''\n",
    "    Function that extracts all texts from a list of authors grouped by communities.\n",
    "    \n",
    "    Parameters:\n",
    "    communities (dict): Dictionary where keys are community identifiers and values are lists of authors.\n",
    "    df_tm (DataFrame): DataFrame containing text data with 'author_son', 'author_mother', 'text_son', and 'text_mother' columns.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary where keys are community identifiers and values are lists of unique texts.\n",
    "    '''\n",
    "    \n",
    "    # Initialize a dictionary to store texts for each community\n",
    "    communities_texts = {}\n",
    "    \n",
    "    # Iterate over each community and its list of authors\n",
    "    for community, authors in communities.items():\n",
    "        # Initialize a set to store unique texts for the current community\n",
    "        texts = set()\n",
    "        \n",
    "        # Iterate over DataFrame rows\n",
    "        for ind, obs in df_tm.iterrows():\n",
    "            # If the author is in the list of authors for the current community, add the text to the set\n",
    "            if obs.author_son in authors:\n",
    "                texts.add(obs.text_son)\n",
    "            if obs.author_mother in authors:\n",
    "                texts.add(obs.text_mother)\n",
    "        \n",
    "        # Convert the set of texts to a list and store it in the dictionary\n",
    "        # Sorting for reproducibility\n",
    "        communities_texts[community] = sorted(list(texts))\n",
    "\n",
    "        if verbose:\n",
    "            # Print the number of unique texts for the current community\n",
    "            print(f\"Community {community} has {len(communities_texts[community])} unique texts.\")\n",
    "    \n",
    "    return communities_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61587251-baa8-4a30-ab10-e96da9a6d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "def clean_texts(communities_texts):\n",
    "    '''\n",
    "    Function that cleans texts and groups them by community.\n",
    "    \n",
    "    Parameters:\n",
    "    communities_texts (dict): Dictionary where keys are community identifiers and values are lists of texts.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary where keys are community identifiers and values are lists of cleaned texts.\n",
    "    '''\n",
    "    \n",
    "    def clean(text):\n",
    "        '''\n",
    "        Function that cleans a given text by lemmatizing and removing unwanted characters.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to be cleaned.\n",
    "        \n",
    "        Returns:\n",
    "        str: The cleaned text.\n",
    "        '''\n",
    "        # First step\n",
    "        text = ' '.join([token.lemma_ \n",
    "                         for token in nlp(text) \n",
    "                         if token.pos_ in {'NOUN', 'ADJ', 'VERB', 'PROPN'}])\n",
    "        \n",
    "        # Second step\n",
    "        text = text.lower()\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "        return text\n",
    "    \n",
    "    # Initialize a dictionary to store cleaned texts for each community\n",
    "    communities_cleaned = {}\n",
    "    \n",
    "    # Iterate over each community and its list of texts\n",
    "    for community, texts in communities_texts.items():\n",
    "        # Clean each text in the list\n",
    "        cleaned_texts = [clean(text) for text in texts]\n",
    "        \n",
    "        # Store the cleaned texts in the dictionary\n",
    "        communities_cleaned[community] = cleaned_texts\n",
    "    \n",
    "    return communities_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083e2813-5a6b-4361-bed0-dc5112503186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling for each community\n",
    "def topics_to_communities(communities_cleaned, umap_model):\n",
    "    '''\n",
    "    Function that fits a BERTopic model to the cleaned texts for each community.\n",
    "    \n",
    "    Parameters:\n",
    "    communities_cleaned (dict): Dictionary where keys are community identifiers and values are lists of cleaned texts.\n",
    "    umap_model (umap.UMAP): A pre-trained UMAP model to be used with BERTopic.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary where keys are community identifiers and values are df from top5 BERTopic models fitted to the community texts.\n",
    "    '''\n",
    "    \n",
    "    # Initialize a dictionary to store BERTopic models for each community\n",
    "    communities_topics = {}\n",
    "    \n",
    "    # Iterate over each community and its list of cleaned texts\n",
    "    for community, texts in communities_cleaned.items():\n",
    "        # Create and fit a BERTopic model for the current community\n",
    "        topic_model = BERTopic(verbose=False, umap_model=umap_model)\n",
    "        topic_model.fit_transform(texts)\n",
    "        \n",
    "        # Store the fitted BERTopic model in the dictionary\n",
    "        communities_topics[community] = topic_model.get_topic_info().loc[0:5, :]\n",
    "    \n",
    "    return communities_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bede9d6-300f-4dc5-a24c-743dc680cb13",
   "metadata": {},
   "source": [
    "# Generating the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f18e3eb-ad2e-47fe-b21d-72e8128d1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_and_apply(data, communities, date1, date2):\n",
    "    '''\n",
    "    Function that applies topic modeling to each communities, in a given time period\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): DataFrame containing extracted posts and comments from r/IsraelPalestinr\n",
    "    communities: (dict): Dictionary where keys are community identifiers and values are lists of authors.\n",
    "    date1 (string, format: YYYY-MM-DD): date from which the analysis should start\n",
    "    date2 (string, format: YYYY-MM-DD): date from which the analysis should end\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary where keys are community identifiers and values are DataFrames containing Topic Modeling results.\n",
    "    '''\n",
    "\n",
    "    # Convert to datetime object\n",
    "    date1 = pd.to_datetime(date1)\n",
    "    date2 = pd.to_datetime(date2)\n",
    "\n",
    "    # Filter the DataFrame \n",
    "    df_tm = data[(data['son_date'] > date1) & (data['mother_date'] < date2) &\n",
    "        (data['son_date'] > date1) & (data['mother_date'] < date2)]\n",
    "\n",
    "    # Extract texts from Df\n",
    "    communities_texts = extract_texts(communities, df_tm)\n",
    "\n",
    "    # Cleaning texts\n",
    "    communities_cleaned = clean_texts(communities_texts)\n",
    "\n",
    "    # Applying topic modeling\n",
    "    communities_topics = topics_to_communities(communities_cleaned, umap_model)\n",
    "\n",
    "    return communities_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bd554dc-ec11-42bb-b2da-ff8ac850587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 0 has 11183 unique texts.\n",
      "Community 5 has 8089 unique texts.\n",
      "Community 2 has 5308 unique texts.\n",
      "Community 3 has 9041 unique texts.\n",
      "Community 4 has 8890 unique texts.\n",
      "Community 1 has 10243 unique texts.\n",
      "Community 6 has 6986 unique texts.\n",
      "Community 7 has 6319 unique texts.\n"
     ]
    }
   ],
   "source": [
    "output = subset_and_apply(data, communities, '2023-10-07', '2023-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d021b0e-ffc1-4ca6-9c02-c87a682af0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json(dict_of_dfs, json_file_path):\n",
    "    '''\n",
    "    Function to export a dictionary containing DataFrames as values to a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    dict_of_dfs (dict): Dictionary where keys are identifiers and values are DataFrames.\n",
    "    json_file_path (str): Path to the output JSON file.\n",
    "    '''\n",
    "    \n",
    "    # Initialize a dictionary to hold JSON-compatible data\n",
    "    dict_of_json = {}\n",
    "    \n",
    "    # Convert each DataFrame to a dictionary\n",
    "    for key, df in dict_of_dfs.items():\n",
    "        dict_of_json[key] = df.to_dict(orient='records')\n",
    "    \n",
    "    # Serialize the dictionary to a JSON file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(dict_of_json, json_file, indent=4)\n",
    "        \n",
    "\n",
    "# Export the dictionary to a JSON file\n",
    "export_to_json(output, 'topic_1007_to_1231.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae05198-a034-4dd9-ae8c-b4785c9d27bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ae3f4e9-0b8b-419c-97c4-e365613498d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_from_json(json_file_path):\n",
    "    '''\n",
    "    Function to import a dictionary containing DataFrames from a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    json_file_path (str): Path to the input JSON file.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary where keys are identifiers and values are DataFrames.\n",
    "    '''\n",
    "    \n",
    "    # Read the JSON file into a dictionary\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        dict_of_json = json.load(json_file)\n",
    "    \n",
    "    # Convert each JSON-compatible structure back into a DataFrame\n",
    "    dict_of_dfs = {key: pd.DataFrame(value) for key, value in dict_of_json.items()}\n",
    "    \n",
    "    return dict_of_dfs\n",
    "\n",
    "\n",
    "topic_example1 = import_from_json('topic_1007_to_1231.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa481e16-038d-4c38-94a8-838ec27d2eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['israel',\n",
       " 'hamas',\n",
       " 'have',\n",
       " 'people',\n",
       " 'palestinians',\n",
       " 'say',\n",
       " 'jews',\n",
       " 'gaza',\n",
       " 'war',\n",
       " 'kill']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_example1[\"6\"].iloc[0].Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555c5c9-de1d-4d1b-9f88-ccc85ccd5380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
